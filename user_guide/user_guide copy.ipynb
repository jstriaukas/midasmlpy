{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import midasmlpy.date_functions as datef # used to handle different frequencies of data and to create lags\n",
    "# import midasmlpy.sparse_group_lasso as sgl # used to run the sparse group lasso and related functions\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sglfitF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from xlsx files and create a dataframe\n",
    "Predictors = pd.read_excel('/Users/m.egelundmuller/Documents/GitHub/midasmlpy/user_guide/predictors-monthly.xlsx').to_numpy()\n",
    "Target = pd.read_excel('/Users/m.egelundmuller/Documents/GitHub/midasmlpy/user_guide/gdp-quarterly.xlsx').to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into dates and data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y data and X and Y dates can also be defined as they are the same for all iterations\n",
    "Y_date = Target[:,0]\n",
    "Y = Target[:,1]\n",
    "X_date = Predictors[:,0]\n",
    "X = Predictors[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data using functions from data_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables ued in transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag variables\n",
    "x_lags = 3\n",
    "y_lags = 0\n",
    "horizon = 0\n",
    "\n",
    "# Legendre matrix\n",
    "degree = 4 # 3 degrees + polynomial 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call data transformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = datef.data_transform(Y, Y_date, X, X_date, x_lags, y_lags, horizon, degree=degree, standardize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = transformed_data['X_tilde']\n",
    "y = transformed_data['Y']\n",
    "\n",
    "# # Split x and y into a 80/20 train test split\n",
    "train_size = int(0.8*x.shape[0])\n",
    "x_train, x_test = x[:train_size], x[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sgLasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sparsegllog_compiled # the sparse group lasso module from fortran\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import random\n",
    "random.seed(111)\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "########### Functions related to fitting the sparse group lasso model.##################\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "def calc_gamma(x, ix, iy, bn):\n",
    "    \"\"\"\n",
    "    Calculates a measure (gamma) for columns of matrix 'x' specified by ranges in 'ix' and 'iy'.\n",
    "    \"\"\"\n",
    "    gamma = np.full(bn, np.nan)\n",
    "    for g in range(bn):\n",
    "        grabcols = slice(ix[g], iy[g] + 1)  # Python uses 0-based indexing\n",
    "        submatrix = x[:, grabcols]\n",
    "        ncols = submatrix.shape[1]\n",
    "        \n",
    "        if ncols > 2:\n",
    "            # Calculate the largest singular value squared \n",
    "            singular_values = svds(submatrix, k=1, return_singular_vectors=False, random_state = 42)\n",
    "            gamma[g] = singular_values[0]**2\n",
    "        elif ncols == 2:\n",
    "            # Returns the largest squared singular value of a n-by-2 matrix x\n",
    "            mat = np.dot(submatrix.T, submatrix)\n",
    "            tr = mat[0, 0] + mat[1, 1]\n",
    "            det = mat[0, 0] * mat[1, 1] - mat[0, 1] * mat[1, 0] \n",
    "            gamma[g] = (tr + np.sqrt(tr**2 - 4 * det)) / 2\n",
    "        else:\n",
    "            gamma[g] = np.sum(submatrix**2)\n",
    "    \n",
    "    return gamma / x.shape[0]\n",
    "\n",
    "\n",
    "def sgLasso_estimation(x, y, group_size, alsparse, family, pmax = 100, intr = True, nlam=None, ulam=None):\n",
    "    \"\"\"\n",
    "    Implements the Sparse Group Lasso algorithm, which is a regularization technique combining \n",
    "    both lasso (L1) and group lasso (L2) penalties. This method is particularly useful for \n",
    "    scenarios where both individual feature sparsity and group sparsity are desired. The \n",
    "    algorithm assumes that features are divided into groups of equal size.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): A (n_obs, n_features) matrix of input features, where n_obs is the number of observations and \n",
    "                           n_features is the total number of features.\n",
    "        y (numpy.ndarray): A vector of length n_obs that contains the dependent variable (response variable).\n",
    "        group_size (int): The number of features in each group. It is assumed that all groups have the same number of features.\n",
    "        alsparse (float): The alpha parameter that balances the L1 and L2 penalties. An alsparse close to 1.0 gives more weight to the \n",
    "                          L1 part (similar to Lasso), while an alsparse close to 0.0 gives more weight to the L2 part (similar to Group Lasso).\n",
    "        nlam (int, optional): The number of lambda values to use for fitting the model. Default is 100.\n",
    "        ulam (numpy.ndarray, optional): A sequence of lambda values to use for fitting the model. Default is np.ones(nlam).\n",
    "        pmax (int, optional): The maximum number of non-zero coefficients allowed in the model. Default is 100.\n",
    "        intr (bool, optional): If True, an intercept is included in the model. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains the following elements:\n",
    "            nalam (int): The number of lambda values actually used.\n",
    "            b0 (numpy.ndarray): The estimated intercept (if intr is True).\n",
    "            beta (numpy.ndarray): Coefficient estimates for the predictors.\n",
    "            activeGroup (numpy.ndarray): Indices of active groups in the model.\n",
    "            nbeta (int): Number of active predictors.\n",
    "            alam (numpy.ndarray): The sequence of lambda values used.\n",
    "            npass (int): The number of passes (iterations) over the data.\n",
    "            jerr (int): An error code (if any) from the fitting process (0 means no error).\n",
    "    \"\"\"\n",
    "    if ulam is not None:\n",
    "        ulam = np.array(ulam)\n",
    "        nlam = len(ulam)  # Override nlam based on the length of ulam if ulam is provided\n",
    "    elif nlam is None:\n",
    "        nlam = 100  # Default value if neither ulam nor nlam is provided\n",
    "\n",
    "    if ulam is None:\n",
    "        ulam = np.ones(nlam)  # Default ulam if not provided\n",
    "\n",
    "    nobs,nvars = x.shape[0], x.shape[1] # Number of observations and features\n",
    "    eps = 1e-8 # Convergence threshold\n",
    "    maxit = 1000000 # Maximum number of iterations\n",
    "    bn = x.shape[1]//group_size # Number of groups as an integer\n",
    "    bs = np.full(bn, group_size, dtype=int) # Elements in groups\n",
    "    ix, iy =  np.array(range(0, nvars, group_size)), np.array(range(group_size-1, nvars, group_size)) # Placement og first column of each group in x\n",
    "    pf, pfl1 = np.sqrt(bs),np.ones(nvars) # Penalty factors for L2 and L1 penalties\n",
    "    dfmax = bn + 1 # Maximum number of groups\n",
    "    flmin = 0.01 if nobs < nvars else 1e-04\n",
    "    lb,ub = np.full(bn, -np.inf),np.full(bn, np.inf) # Lower and upper bounds for the coefficients\n",
    "    \n",
    "    if family == 'binomial':\n",
    "        gam = 0.25 * calc_gamma(x, ix, iy, bn) # Calculate gamma values for each group of features (columns) \n",
    "        _nalam, b0, beta, _activeGroup, _nbeta, alam, npass, jerr = sparsegllog_compiled.log_sparse_four(x = x,\n",
    "                        y = y, bn = bn, bs = bs, \n",
    "                        ix = ix + 1, iy = iy + 1, # iy and ix are +1 as fortran is index 1 while python is index 0\n",
    "                        gam = gam, nobs = nobs, \n",
    "                        nvars = nvars, pf = pf, pfl1 = pfl1, dfmax = dfmax, pmax = pmax, \n",
    "                        nlam = nlam, flmin = flmin, ulam = ulam, eps = eps, maxit = maxit, \n",
    "                        intr = intr, lb = lb, ub = ub, alsparse = alsparse)\n",
    "        mse = None # to make it easier to return the same number of variables for all families\n",
    "    if family == 'gaussian':\n",
    "        if intr:\n",
    "            y = y-y.mean()\n",
    "        gam = calc_gamma(x, ix, iy, bn) # Calculate gamma values for each group of features (columns) \n",
    "        _nalam, b0, beta, _activeGroup, _nbeta, alam, npass, jerr, mse = sparsegllog_compiled.sparse_four(x = x,\n",
    "                        y = y, bn = bn, bs = bs, \n",
    "                        ix = ix + 1, iy = iy + 1, # iy and ix are +1 as fortran is index 1 while python is index 0\n",
    "                        gam = gam, nobs = nobs, \n",
    "                        nvars = nvars, pf = pf, pfl1 = pfl1, dfmax = dfmax, pmax = pmax, \n",
    "                        nlam = nlam, flmin = flmin, ulam = ulam, eps = eps, maxit = maxit, \n",
    "                        intr = intr, lb = lb, ub = ub, alsparse = alsparse)\n",
    "    if jerr != 0:\n",
    "        raise ValueError(\"Error in the sparse group lasso estimation.\")\n",
    "    if npass == maxit:\n",
    "        raise ValueError(\"Failed to converge in the sparse group lasso estimation.\")\n",
    "    return b0, beta, alam, npass, jerr, mse\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "######### Functions related to finding the optimal sparse group lasso model.############\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "def evaluate_binomials(x, y, b0, beta,eval = 'auc', threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of several logistic regression models using specified metrics for different values of lambda.\n",
    "\n",
    "    Parameters:\n",
    "    - x (ndarray): A 2D numpy array of input features (identical to `x` in `predict`).\n",
    "    - y (ndarray): A 1D numpy array containing the true binary outcomes (0 or 1) for each sample in `x`.\n",
    "    - b0 (ndarray): A 1D numpy array of intercepts, one for each model being evaluated.\n",
    "    - beta (ndarray): A 2D numpy array where each column corresponds to the coefficients of a model.\n",
    "    - eval (str): The metric for evaluation; 'accuracy' for accuracy score, 'auc' for AUC score.\n",
    "\n",
    "    Returns:\n",
    "    - accuracies (list): If `eval` == 'accuracy', a list of accuracy scores for each model.\n",
    "    - auc_scores (list): If `eval` == 'auc', a list of AUC scores for each model.\n",
    "    \"\"\"\n",
    "    evaluation_score = [0] * len(b0)  # this will store evaluation score\n",
    "    for l in range(len(b0)):\n",
    "        probabilities = 1 / (1 + np.exp(-np.dot(x, beta[:,l]) + b0[l]))\n",
    "        predictions = (probabilities > threshold).astype(int)\n",
    "        if eval == 'accuracy':\n",
    "            evaluation_score[l] = accuracy_score(y, predictions)  \n",
    "        elif eval == 'auc':\n",
    "            evaluation_score[l] = roc_auc_score(y, predictions)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid evaluation metric. Use 'accuracy' or 'auc'.\")\n",
    "    return evaluation_score\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_gaussian(x, y, b0, beta, intr, eval='mse'):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of several linear regression models using specified metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - x (ndarray): A 2D numpy array of input features (identical to `x` in `predict`).\n",
    "    - y (ndarray): A 1D numpy array containing the true continuous outcomes for each sample in `x`.\n",
    "    - b0 (ndarray): A 1D numpy array of intercepts, one for each model being evaluated.\n",
    "    - beta (ndarray): A 2D numpy array where each column corresponds to the coefficients of a model.\n",
    "    - eval (str): The metric for evaluation; options are 'mse' for Mean Squared Error or 'r2' for R-squared.\n",
    "\n",
    "    Returns:\n",
    "    - evaluation_scores (list): A list of scores, either MSE or R-squared, for each model.\n",
    "    \"\"\"\n",
    "    evaluation_scores = [0] * len(b0)  # this will store evaluation scores\n",
    "    for l in range(len(b0)):\n",
    "        predictions = np.dot(x, beta[:,l]) + b0[l]\n",
    "        if intr:  # Adjust predictions if intercept was used during fitting\n",
    "            predictions += mean_y\n",
    "        if eval == 'mse':\n",
    "            evaluation_scores[l] = mean_squared_error(y, predictions)\n",
    "        elif eval == 'r2':\n",
    "            evaluation_scores[l] = r2_score(y, predictions)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid evaluation metric. Use 'mse' or 'r2'.\")\n",
    "    return evaluation_scores\n",
    "\n",
    "\n",
    "def best_lambda_find(x,y,group_size, alsparse, family, nlam = 100, pmax = 100, intr = True,k_folds = 5):\n",
    "    \"\"\"\n",
    "    Find the best model using sparse group lasso. The sparse group lasso finds coefficients for nlam values of lambda, and the best model\n",
    "    is chosen as the one with the highest mean performance in k-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - x (ndarray): A 2D numpy array of input features (identical to `x` in `predict`).\n",
    "    - y (ndarray): A 1D numpy array containing the true binary outcomes (0 or 1) for each sample in `x`.\n",
    "    - group_size (int): The number of lags used for the legendre polynomials.\n",
    "    - alsparse (int): The balancing parameter alpha.\n",
    "    - nlam (int, optional): The number of lambda values to evaluate (defaults to 100).\n",
    "    - pmax (int, optional): The maximum number of non-zero coefficients allowed in the model (defaults to 100).\n",
    "    - intr (bool, optional): If True, an intercept is included in the model (defaults to True).\n",
    "    - k_folds (int, optional): The number of folds for cross-validation (defaults to 5).\n",
    "\n",
    "    Returns:\n",
    "    - best_model (dict): A dictionary containing the following\n",
    "        - 'b0' (float): The intercept of the best model.\n",
    "        - 'beta' (ndarray): The coefficients of the best model.\n",
    "        - 'maximized_performance' (float): The maximized performance of the best model.\n",
    "        - 'best_lambda' (float): The lambda value of the best model.\n",
    "    \"\"\"\n",
    "    # Find model nlam number of models\n",
    "    b0, beta, alam, _npass, _jerr, mse = sgLasso_estimation(x, y, group_size, alsparse,family, pmax, intr)\n",
    "\n",
    "    # Find mean performance for each lambda\n",
    "    # Split the data into k_folds\n",
    "    if family == 'binomial':\n",
    "        kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    if family == 'gaussian':   \n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # initialize performance list\n",
    "    performance = []\n",
    "    for train_index, test_index in kf.split(x,y):\n",
    "        # Based on the split, create the training and test data for this fold\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Estimate the model on the training data\n",
    "        b0test, betatest, _alam, _npass, _jerr, msetrain = sgLasso_estimation(x_train, y_train, group_size, alsparse, family, pmax = pmax, intr = intr, ulam = alam)\n",
    "        if family == 'gaussian':\n",
    "            performance.append(evaluate_gaussian(x_test, y_test, b0test, betatest,intr,eval = 'mse'))\n",
    "        if family == 'binomial':\n",
    "            performance.append(evaluate_binomials(x_test, y_test, b0test, betatest,eval = 'auc', threshold=0.5))\n",
    "\n",
    "    performance = np.array(performance)\n",
    "    mean_performance = np.mean(performance, axis=0)\n",
    "    if family == 'gaussian':\n",
    "        best_lambda = np.argmin(mean_performance)\n",
    "    if family == 'binomial':\n",
    "        best_lambda = np.argmax(mean_performance)\n",
    "    return {'b0': b0[best_lambda], \n",
    "            'beta': beta[:,best_lambda], \n",
    "            'best_performance': mean_performance[best_lambda], \n",
    "            'best_lambda': alam[best_lambda]}\n",
    "    \n",
    "def best_model(x, y, group_size, family, nlam = 100, pmax = 100, intr = True, k_folds = 5, disp_flag = True, alpha_values = None, alpha = None):\n",
    "    \"\"\"\n",
    "    Function to find the best model based on the maximized performance of the model. The function uses the bestlambda function to find the best lambda value for the model.\n",
    "\n",
    "    Parameters:\n",
    "    x: numpy array\n",
    "        The predictors for the model\n",
    "    y: numpy array\n",
    "        The target variable for the model\n",
    "    group_size: int\n",
    "        The number of groups in the model\n",
    "    nlam: int\n",
    "        The number of lambda values to test\n",
    "    pmax: int\n",
    "        The maximum number of variables in the model\n",
    "    intr: bool\n",
    "        Whether to include the intercept in the model\n",
    "    k_folds: int\n",
    "        The number of folds to use in the cross-validation\n",
    "    disp_flag: bool\n",
    "        Whether to display the performance at different values of alpha\n",
    "    alpha_values: int\n",
    "        The number of alpha values to test\n",
    "    alpha: list\n",
    "        The alpha value to test\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        A dictionary containing the best alpha value, the best performance, the intercept and the coefficients of the model\n",
    "    \"\"\"\n",
    "    if alpha is not None:\n",
    "        alsparse_values = np.array(alpha)\n",
    "    elif alpha_values is not None:\n",
    "        alsparse_values = np.linspace(1, 0, alpha_values)\n",
    "    else:\n",
    "        alsparse_values = np.linspace(1, 0, 5)\n",
    "\n",
    "    # Dictionary to store the average maximized performances for each alsparse\n",
    "    if disp_flag:\n",
    "        performance_dict = {}\n",
    "    best_performance = None\n",
    "    best_alsparse = None\n",
    "    b0, beta = None, None  # Initialize parameters that will store best model coefficients\n",
    "\n",
    "    # Cross-validation process\n",
    "    for alsparse in alsparse_values:\n",
    "        model_result = best_lambda_find(x,y,group_size, alsparse, family, nlam = nlam, pmax = pmax, intr = intr,k_folds = k_folds)\n",
    "        # Append the maximized performance of this fold\n",
    "        if disp_flag:\n",
    "            performance_dict[alsparse] = model_result['best_performance'].round(5)\n",
    "        # If this fold has a higher maximized performance than the previous best, update the best performance\n",
    "        if best_performance is None:\n",
    "                best_performance = model_result['best_performance']\n",
    "                best_alsparse = alsparse\n",
    "                b0 = model_result['b0']\n",
    "                beta = model_result['beta'] \n",
    "                best_lambda = model_result['best_lambda']\n",
    "        else:\n",
    "            if family == 'gaussian':\n",
    "                if model_result['best_performance']<best_performance:\n",
    "                    best_performance = model_result['best_performance']\n",
    "                    best_alsparse = alsparse\n",
    "                    b0 = model_result['b0']\n",
    "                    beta = model_result['beta'] \n",
    "                    best_lambda = model_result['best_lambda']\n",
    "            if family == 'binomial':\n",
    "                if model_result['best_performance']>best_performance:\n",
    "                    best_performance = model_result['best_performance']\n",
    "                    best_alsparse = alsparse\n",
    "                    b0 = model_result['b0']\n",
    "                    beta = model_result['beta'] \n",
    "                    best_lambda = model_result['best_lambda']\n",
    "\n",
    "    if disp_flag:\n",
    "        print('The performance at different values of alpha are:')\n",
    "        print(performance_dict)\n",
    "    \n",
    "    return {'best_alsparse':best_alsparse, \n",
    "            'best_performance':best_performance, \n",
    "            'b0':b0, \n",
    "            'beta':beta,\n",
    "            'best_lambda': best_lambda}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance at different values of alpha are:\n",
      "{1.0: 12.0668, 0.9: 12.21292, 0.8: 12.31221, 0.7: 12.38778, 0.6: 12.4934, 0.5: 12.56949, 0.3999999999999999: 12.62666, 0.29999999999999993: 12.71384, 0.19999999999999996: 12.82012, 0.09999999999999998: 12.88895, 0.0: 12.95272}\n"
     ]
    }
   ],
   "source": [
    "model2 = best_model(x = x_train, y = y_train, group_size = degree, family = 'gaussian', nlam = 100, pmax = 122, intr = False, k_folds = 3, disp_flag = True, alpha_values = 11, alpha = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.06679602547654"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2['best_performance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance at different values of alpha are:\n",
      "{1.0: 12.0668, 0.9: 12.21292, 0.8: 12.31221, 0.7: 12.38778, 0.6: 12.4934, 0.5: 12.56949, 0.3999999999999999: 12.62666, 0.29999999999999993: 12.71384, 0.19999999999999996: 12.82012, 0.09999999999999998: 12.88895, 0.0: 12.95272}\n"
     ]
    }
   ],
   "source": [
    "model2 = best_model(x = x_train, y = y_train, group_size = degree, family = 'gaussian', nlam = 100, pmax = 122, intr = False, k_folds = 3, disp_flag = True, alpha_values = 11, alpha = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.06679602547654"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2['best_performance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_train\n",
    "y = y_train\n",
    "group_size = degree\n",
    "family = 'gaussian'\n",
    "nlam = 100\n",
    "pmax = 122\n",
    "intr = False\n",
    "k_folds = 3\n",
    "disp_flag = True\n",
    "alpha_values = 11\n",
    "alpha = None\n",
    "alsparse = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate_gaussian() missing 1 required positional argument: 'intr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m b0test, betatest, _alam, _npass, _jerr, msetrain \u001b[38;5;241m=\u001b[39m sgLasso_estimation(x_train, y_train, group_size, alsparse, family, pmax \u001b[38;5;241m=\u001b[39m pmax, intr \u001b[38;5;241m=\u001b[39m intr, ulam \u001b[38;5;241m=\u001b[39m alam)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m family \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgaussian\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     performance\u001b[38;5;241m.\u001b[39mappend(\u001b[43mevaluate_gaussian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb0test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetatest\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m family \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinomial\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     22\u001b[0m     performance\u001b[38;5;241m.\u001b[39mappend(evaluate_binomials(x_test, y_test, b0test, betatest,\u001b[38;5;28meval\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate_gaussian() missing 1 required positional argument: 'intr'"
     ]
    }
   ],
   "source": [
    "    # Find model nlam number of models\n",
    "    b0, beta, alam, _npass, _jerr, mse = sgLasso_estimation(x, y, group_size, alsparse,family, pmax, intr)\n",
    "\n",
    "    # Find mean performance for each lambda\n",
    "    # Split the data into k_folds\n",
    "    if family == 'binomial':\n",
    "        kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    if family == 'gaussian':   \n",
    "        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # initialize performance list\n",
    "    performance = []\n",
    "    for train_index, test_index in kf.split(x,y):\n",
    "        # Based on the split, create the training and test data for this fold\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Estimate the model on the training data\n",
    "        b0test, betatest, _alam, _npass, _jerr, msetrain = sgLasso_estimation(x_train, y_train, group_size, alsparse, family, pmax = pmax, intr = intr, ulam = alam)\n",
    "        if family == 'gaussian':\n",
    "            performance.append(evaluate_gaussian(x_test, y_test, b0test, betatest,eval = 'mse'))\n",
    "        if family == 'binomial':\n",
    "            performance.append(evaluate_binomials(x_test, y_test, b0test, betatest,eval = 'auc', threshold=0.5))\n",
    "\n",
    "    performance = np.array(performance)\n",
    "    mean_performance = np.mean(performance, axis=0)\n",
    "    if family == 'binomial':\n",
    "        best_lambda = np.argmax(mean_performance)\n",
    "    if family == 'gaussian':\n",
    "        best_lambda = np.argmin(mean_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.72073806, 15.68469278, 15.49494481, 15.32664924, 15.1753788 ,\n",
       "       15.0394381 , 14.91728718, 14.79831989, 14.69161439, 14.58669592,\n",
       "       14.4925171 , 14.40975478, 14.32792115, 14.23311733, 14.0924809 ,\n",
       "       13.97235306, 13.86846429, 13.77427229, 13.6950444 , 13.62711568,\n",
       "       13.57304908, 13.52009265, 13.45608846, 13.37643942, 13.3041933 ,\n",
       "       13.24390963, 13.20272572, 13.17317758, 13.16261396, 13.16207891,\n",
       "       13.15776153, 13.15411543, 13.15249136, 13.13512304, 13.11654678,\n",
       "       13.09464479, 13.05082401, 12.98869776, 12.91041804, 12.84006636,\n",
       "       12.77987338, 12.7202505 , 12.6619418 , 12.61368418, 12.58416023,\n",
       "       12.56948842, 12.58089292, 12.60514016, 12.64270585, 12.68403573,\n",
       "       12.74605675, 12.81651275, 12.9162938 , 13.02654741, 13.14018596,\n",
       "       13.24087544, 13.3647866 , 13.50583955, 13.65282761, 13.81935746,\n",
       "       13.99596268, 14.17420344, 14.36136385, 14.54756812, 14.71971168,\n",
       "       14.88544037, 15.07012714, 15.26097823, 15.45872708, 15.66306066,\n",
       "       15.85485989, 16.02871261, 16.21246367, 16.40603603, 16.63416001,\n",
       "       16.87099541, 17.12980148, 17.39294038, 17.66166373, 17.93438411,\n",
       "       18.20711402, 18.47746623, 18.74020563, 19.00437259, 19.25848625,\n",
       "       19.50350408, 19.73735056, 19.96218899, 20.18471506, 20.39528205,\n",
       "       20.58894653, 20.78371767, 20.9737602 , 21.15971725, 21.33795602,\n",
       "       21.52365682, 21.7117016 , 21.9017875 , 22.09304138, 22.28175701])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.569488420636413"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_performance[best_lambda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(mean_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = transformed_data['X_tilde']\n",
    "y = transformed_data['Y']\n",
    "\n",
    "# # Split x and y into a 80/20 train test split\n",
    "train_size = int(0.8*x.shape[0])\n",
    "x_train, x_test = x[:train_size], x[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0, beta, alam, _npass, _jerr, mse = sgLasso_estimation(x_train, y_train, group_size, alsparse,family, pmax, intr)\n",
    "evaluation_scores = [0] * len(b0)  # this will store evaluation scores\n",
    "for l in range(len(b0)):\n",
    "    predictions = np.dot(x_test, beta[:,l]) + b0[l]\n",
    "    evaluation_scores[l] = mean_squared_error(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66.67973619271427,\n",
       " 66.66931066055061,\n",
       " 66.59851725606049,\n",
       " 66.5317382351875,\n",
       " 66.46871618002568,\n",
       " 66.40920931545486,\n",
       " 66.3529970300571,\n",
       " 66.29987845217595,\n",
       " 66.24967026451827,\n",
       " 66.20220621963709,\n",
       " 66.15733641090557,\n",
       " 66.1149261202311,\n",
       " 66.07485511895634,\n",
       " 66.03701712057271,\n",
       " 66.00131938223261,\n",
       " 65.95432307928232,\n",
       " 65.902637478755,\n",
       " 65.86062823728464,\n",
       " 65.81928510611839,\n",
       " 65.77666324237987,\n",
       " 65.72177092426341,\n",
       " 65.71595239753776,\n",
       " 65.74122524773539,\n",
       " 65.76844263013173,\n",
       " 65.71892815299486,\n",
       " 65.67069136843438,\n",
       " 65.63721084185885,\n",
       " 65.61596415275122,\n",
       " 65.64115886486319,\n",
       " 65.67454284698647,\n",
       " 65.82231958955516,\n",
       " 66.00603189368738,\n",
       " 66.15102225346607,\n",
       " 66.25443014565182,\n",
       " 66.32635088372936,\n",
       " 66.41786136425802,\n",
       " 66.41424197901995,\n",
       " 66.39330802493555,\n",
       " 66.43509613506544,\n",
       " 66.5046442677282,\n",
       " 66.58454308107342,\n",
       " 66.66999263480321,\n",
       " 66.75863384605294,\n",
       " 66.851708892422,\n",
       " 67.00356943408615,\n",
       " 67.19962874767405,\n",
       " 67.40932367352032,\n",
       " 67.64931674004487,\n",
       " 67.91035460480109,\n",
       " 68.18692489683862,\n",
       " 68.46569275046427,\n",
       " 68.75020797181136,\n",
       " 68.8296770129149,\n",
       " 68.91734680402662,\n",
       " 69.06425107673695,\n",
       " 69.23496996457142,\n",
       " 69.3935989889704,\n",
       " 69.55412723900638,\n",
       " 69.6990617092438,\n",
       " 69.83237130799999,\n",
       " 69.9707686492997,\n",
       " 70.11629534350983,\n",
       " 70.27244263104468,\n",
       " 70.49272886609931,\n",
       " 70.83186820474403,\n",
       " 71.18766989162563,\n",
       " 71.54240074127942,\n",
       " 71.8626300161896,\n",
       " 72.14163622918255,\n",
       " 72.42509460089889,\n",
       " 72.75676568251794,\n",
       " 73.03831898291388,\n",
       " 73.33315825311918,\n",
       " 73.64904733820829,\n",
       " 74.0088373221701,\n",
       " 74.37771647205665,\n",
       " 74.75674444698893,\n",
       " 75.12424268075381,\n",
       " 75.5013691969923,\n",
       " 75.88414878850769,\n",
       " 76.26145924437725,\n",
       " 76.63743812985047,\n",
       " 77.01678011153976,\n",
       " 77.39195224958507,\n",
       " 77.68666165116912,\n",
       " 77.92106997207135,\n",
       " 78.09982665940966,\n",
       " 78.2654782150305,\n",
       " 78.41815780068755,\n",
       " 78.56527635653057,\n",
       " 78.74469498700704,\n",
       " 78.92863316318248,\n",
       " 79.01516461546402,\n",
       " 78.98704616148066,\n",
       " 78.83775447607631,\n",
       " 78.77523114769086,\n",
       " 78.78292626714425,\n",
       " 78.75708509228757,\n",
       " 78.74567407048067,\n",
       " 78.72430972858643]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3.58582</th>\n",
       "      <td>1.033435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.46998</th>\n",
       "      <td>5.012366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.59778</th>\n",
       "      <td>2.844964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.73724</th>\n",
       "      <td>6.968606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.31139</th>\n",
       "      <td>10.076522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.28244</th>\n",
       "      <td>5.805969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.82754</th>\n",
       "      <td>5.754215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.21107</th>\n",
       "      <td>3.512108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.94283</th>\n",
       "      <td>4.712126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.23324</th>\n",
       "      <td>7.100055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.14173</th>\n",
       "      <td>-0.163556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.48297</th>\n",
       "      <td>3.273559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.24048</th>\n",
       "      <td>-3.609681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.11768</th>\n",
       "      <td>-1.737373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.48704</th>\n",
       "      <td>-1.361330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.56618</th>\n",
       "      <td>-0.597880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.16653</th>\n",
       "      <td>1.177792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.30498</th>\n",
       "      <td>-0.309984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.50326</th>\n",
       "      <td>0.971633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.55708</th>\n",
       "      <td>5.426327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-5.48895</th>\n",
       "      <td>2.510756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-32.87910</th>\n",
       "      <td>2.898383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.89166</th>\n",
       "      <td>0.008153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.11961</th>\n",
       "      <td>2.684946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.10931</th>\n",
       "      <td>1.183741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.03326</th>\n",
       "      <td>5.224906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.24430</th>\n",
       "      <td>1.680657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.73015</th>\n",
       "      <td>3.293399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1.99569</th>\n",
       "      <td>0.141799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0.56549</th>\n",
       "      <td>5.809787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.62584</th>\n",
       "      <td>-2.841674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.53365</th>\n",
       "      <td>-0.770768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.21935</th>\n",
       "      <td>-2.649608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.03928</th>\n",
       "      <td>8.018051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.74720</th>\n",
       "      <td>-1.547340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       " 3.58582    1.033435\n",
       " 2.46998    5.012366\n",
       " 1.59778    2.844964\n",
       " 0.73724    6.968606\n",
       " 2.31139   10.076522\n",
       " 1.28244    5.805969\n",
       " 2.82754    5.754215\n",
       " 2.21107    3.512108\n",
       " 1.94283    4.712126\n",
       " 2.23324    7.100055\n",
       " 3.14173   -0.163556\n",
       " 4.48297    3.273559\n",
       " 3.24048   -3.609681\n",
       " 2.11768   -1.737373\n",
       " 2.48704   -1.361330\n",
       " 0.56618   -0.597880\n",
       " 2.16653    1.177792\n",
       " 3.30498   -0.309984\n",
       " 4.50326    0.971633\n",
       " 2.55708    5.426327\n",
       "-5.48895    2.510756\n",
       "-32.87910   2.898383\n",
       " 29.89166   0.008153\n",
       " 4.11961    2.684946\n",
       " 5.10931    1.183741\n",
       " 6.03326    5.224906\n",
       " 3.24430    1.680657\n",
       " 6.73015    3.293399\n",
       "-1.99569    0.141799\n",
       "-0.56549    5.809787\n",
       " 2.62584   -2.841674\n",
       " 2.53365   -0.770768\n",
       " 2.21935   -2.649608\n",
       " 2.03928    8.018051\n",
       " 4.74720   -1.547340"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(predictions,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.726799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.695337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.484573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.291053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.113180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.482967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.451148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.421517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.393788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.367055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0   15.726799\n",
       "1   15.695337\n",
       "2   15.484573\n",
       "3   15.291053\n",
       "4   15.113180\n",
       "..        ...\n",
       "95   0.482967\n",
       "96   0.451148\n",
       "97   0.421517\n",
       "98   0.393788\n",
       "99   0.367055\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(evaluation_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
